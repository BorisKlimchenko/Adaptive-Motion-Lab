{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Initialize Workspace & Hardware Profile\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "REPO_URL = \"https://github.com/BorisKlimchenko/Adaptive-Motion-Lab.git\"\n",
    "REPO_DIR = \"/content/Adaptive-Motion-Lab\"\n",
    "\n",
    "# 1. Secure Drive Mount\n",
    "if not os.path.exists('/content/drive'):\n",
    "    print(\"ðŸ“‚ Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"âœ… Drive already mounted.\")\n",
    "\n",
    "# 2. Repository Synchronization\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(f\"â¬‡ï¸ Cloning Repository: {REPO_URL}...\")\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(\"ðŸ”„ Repository exists. Pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# 3. Hardware Profiling\n",
    "print(\"\\n--- Hardware Acceleration Profile ---\")\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"âœ… GPU Detected: {device_name} ({vram:.2f} GB VRAM)\")\n",
    "    \n",
    "    if \"A100\" in device_name:\n",
    "        print(\"ðŸš€ Status: DATA CENTER MODE (High Performance)\")\n",
    "    elif \"T4\" in device_name:\n",
    "        print(\"âš ï¸ Status: EFFICIENCY MODE (Restricted VRAM)\")\n",
    "else:\n",
    "    print(\"âŒ CRITICAL: No GPU detected. Change Runtime Type to GPU.\")\n",
    "\n",
    "# Set working directory context\n",
    "%cd {REPO_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4137e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Smart Environment Orchestrator (Auto-Repair)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import pkg_resources\n",
    "\n",
    "# --- CONFIGURATION (Single Source of Truth) ---\n",
    "# Defines the Golden Standard for reproducibility.\n",
    "TARGET_ENV = {\n",
    "    \"torch\": \"2.5.1+cu121\",\n",
    "    \"torchvision\": \"0.20.1+cu121\",\n",
    "    \"torchaudio\": \"2.5.1+cu121\",\n",
    "    \"xformers\": \"0.0.28.post3\"\n",
    "}\n",
    "\n",
    "# PyTorch Index URL for CUDA 12.1\n",
    "INDEX_URL = \"https://download.pytorch.org/whl/cu121\"\n",
    "\n",
    "def run_command(command, message):\n",
    "    \"\"\"Executes shell command with status logging.\"\"\"\n",
    "    print(f\"ðŸ”„ {message}...\")\n",
    "    try:\n",
    "        subprocess.check_call(command, shell=True)\n",
    "        print(f\"âœ… Success: {message}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Error: {message} failed.\")\n",
    "        raise e\n",
    "\n",
    "def get_installed_version(package_name):\n",
    "    \"\"\"Safely retrieves the installed version.\"\"\"\n",
    "    try:\n",
    "        return pkg_resources.get_distribution(package_name).version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return None\n",
    "\n",
    "def setup_environment():\n",
    "    print(\"ðŸ” Analyzing Runtime Environment...\")\n",
    "    \n",
    "    needs_install = False\n",
    "    current_torch = get_installed_version(\"torch\")\n",
    "    target_base = TARGET_ENV[\"torch\"].split('+')[0]\n",
    "    \n",
    "    # 1. Audit Phase\n",
    "    if current_torch is None:\n",
    "        print(\"âš ï¸ Torch not found. Initialization required.\")\n",
    "        needs_install = True\n",
    "    elif not current_torch.startswith(target_base):\n",
    "        print(f\"âš ï¸ Version Mismatch! Target: {target_base}..., Found: {current_torch}\")\n",
    "        needs_install = True\n",
    "    else:\n",
    "        print(f\"âœ… Core Environment (Torch {current_torch}) is optimized.\")\n",
    "\n",
    "    # 2. Execution Phase\n",
    "    if needs_install:\n",
    "        print(\"ðŸš‘ Initiating Environment Alignment...\")\n",
    "        \n",
    "        # A. Clean Slate Protocol\n",
    "        print(\"   -> Removing conflicting libraries...\")\n",
    "        subprocess.run(\"pip uninstall -y torch torchvision torchaudio xformers\", shell=True, check=False)\n",
    "        \n",
    "        # B. Install Golden Standard\n",
    "        install_cmd = (\n",
    "            f\"pip install \"\n",
    "            f\"torch=={TARGET_ENV['torch']} \"\n",
    "            f\"torchvision=={TARGET_ENV['torchvision']} \"\n",
    "            f\"torchaudio=={TARGET_ENV['torchaudio']} \"\n",
    "            f\"xformers=={TARGET_ENV['xformers']} \"\n",
    "            f\"--index-url {INDEX_URL}\"\n",
    "        )\n",
    "        run_command(install_cmd, \"Installing Core Stack (Torch + xFormers)\")\n",
    "        \n",
    "    # 3. Project Dependencies\n",
    "    run_command(\"pip install -r requirements.txt\", \"Verifying Project Requirements\")\n",
    "\n",
    "    # 4. Path Injection (Fixes 'ModuleNotFoundError' after restart)\n",
    "    if os.getcwd() not in sys.path:\n",
    "        sys.path.append(os.getcwd())\n",
    "        print(f\"ðŸ“‚ System Path Updated: {os.getcwd()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Launch Inference Pipeline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Runtime Path Fix ---\n",
    "# Ensures Python sees the project modules even if Colab context shifted.\n",
    "PROJECT_ROOT = \"/content/Adaptive-Motion-Lab\"\n",
    "if os.path.exists(PROJECT_ROOT):\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    if PROJECT_ROOT not in sys.path:\n",
    "        sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"ðŸš€ Initializing Inference Engine in: {os.getcwd()}\")\n",
    "\n",
    "# --- Execute Main Pipeline ---\n",
    "# Use !python to run as a subprocess, ensuring memory cleanup after run.\n",
    "!python main.py --prompts configs/default_scene.json"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
